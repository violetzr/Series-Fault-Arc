{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db1ea1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-28T08:41:41.291789Z",
     "iopub.status.busy": "2025-02-28T08:41:41.291067Z",
     "iopub.status.idle": "2025-02-28T08:41:41.295547Z",
     "shell.execute_reply": "2025-02-28T08:41:41.294717Z"
    },
    "papermill": {
     "duration": 0.012603,
     "end_time": "2025-02-28T08:41:41.297214",
     "exception": false,
     "start_time": "2025-02-28T08:41:41.284611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use n,x,5000\n",
    "#use 1dcnn\n",
    "#optimism sweep parameter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e81d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:41:41.306960Z",
     "iopub.status.busy": "2025-02-28T08:41:41.306704Z",
     "iopub.status.idle": "2025-02-28T08:41:51.598677Z",
     "shell.execute_reply": "2025-02-28T08:41:51.597723Z"
    },
    "papermill": {
     "duration": 10.298905,
     "end_time": "2025-02-28T08:41:51.600706",
     "exception": false,
     "start_time": "2025-02-28T08:41:41.301801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac057b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:41:51.612219Z",
     "iopub.status.busy": "2025-02-28T08:41:51.611909Z",
     "iopub.status.idle": "2025-02-28T08:42:14.525931Z",
     "shell.execute_reply": "2025-02-28T08:42:14.525050Z"
    },
    "papermill": {
     "duration": 22.922136,
     "end_time": "2025-02-28T08:42:14.527981",
     "exception": false,
     "start_time": "2025-02-28T08:41:51.605845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/lyhue1991/torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a490c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:14.541797Z",
     "iopub.status.busy": "2025-02-28T08:42:14.541211Z",
     "iopub.status.idle": "2025-02-28T08:42:34.368957Z",
     "shell.execute_reply": "2025-02-28T08:42:34.368260Z"
    },
    "papermill": {
     "duration": 19.836478,
     "end_time": "2025-02-28T08:42:34.371142",
     "exception": false,
     "start_time": "2025-02-28T08:42:14.534664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import savgol_filter #滤波\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchkeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773002a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.383990Z",
     "iopub.status.busy": "2025-02-28T08:42:34.383382Z",
     "iopub.status.idle": "2025-02-28T08:42:34.400259Z",
     "shell.execute_reply": "2025-02-28T08:42:34.399401Z"
    },
    "papermill": {
     "duration": 0.025116,
     "end_time": "2025-02-28T08:42:34.402030",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.376914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainmodel VisMetri\n",
    "from torchkeras.utils import is_jupyter\n",
    "class VisMetric:\n",
    "    def __init__(self, figsize=(6, 4), save_path='history.png'):\n",
    "        \"\"\"Visualization callback for monitoring metrics\n",
    "\n",
    "        Args:\n",
    "            figsize (tuple, optional): Figure size. Defaults to (6, 4)\n",
    "            save_path (str, optional): Path to save the history plot. Defaults to 'history.png'\n",
    "        \"\"\"\n",
    "        self.figsize = (6, 4)\n",
    "        self.save_path = save_path\n",
    "        self.in_jupyter = is_jupyter()\n",
    "\n",
    "    def on_fit_start(self, model: 'KerasModel'):\n",
    "        \"\"\"Callback at the beginning of the training\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance.\n",
    "        \"\"\"\n",
    "        if not self.in_jupyter:\n",
    "            print('\\nView dynamic loss/metric plot: \\n' + os.path.abspath(self.save_path))\n",
    "        # //////////////////////////////////////\n",
    "        self.metric = model.monitor.replace('val_', '')\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        x_bounds = [0, min(10, model.epochs)]\n",
    "        title = f'best {model.monitor} = ?'\n",
    "        self.update_graph(model, title=title, x_bounds=x_bounds)\n",
    "\n",
    "    def on_train_epoch_end(self, model: 'KerasModel'):\n",
    "        \"\"\"Callback at the end of each training epoch\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self, model: \"KerasModel\"):\n",
    "        \"\"\"Callback at the end of each validation epoch\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        n = len(dfhistory)\n",
    "        x_bounds = [dfhistory['epoch'].min(), min(10 + (n // 10) * 10, model.epochs)]\n",
    "        title = self.get_title(model)\n",
    "        self.update_graph(model, title=title, x_bounds=x_bounds)\n",
    "\n",
    "    def on_fit_end(self, model: \"KerasModel\"):\n",
    "        \"\"\"Callback at the end of the entire training process\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        title = self.get_title(model)\n",
    "        self.update_graph(model, title=title)\n",
    "\n",
    "    def get_best_score(self, model: 'KerasModel'):\n",
    "        \"\"\"Get the best score and epoch.\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "\n",
    "        Returns:\n",
    "            tuple: Best epoch and best score\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        arr_scores = dfhistory[model.monitor]\n",
    "        best_score = np.max(arr_scores) if model.mode == \"max\" else np.min(arr_scores)\n",
    "        best_epoch = dfhistory.loc[arr_scores == best_score, 'epoch'].tolist()[0]\n",
    "        return (best_epoch, best_score)\n",
    "\n",
    "    def get_title(self, model: 'KerasModel'):\n",
    "        \"\"\"Get the title for the plot\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "\n",
    "        Returns:\n",
    "            str: The title.\n",
    "        \"\"\"\n",
    "        best_epoch, best_score = self.get_best_score(model)\n",
    "        title = f'best {model.monitor}={best_score:.4f} (@epoch {best_epoch})'\n",
    "        return title\n",
    "\n",
    "    def update_graph(self, model: 'KerasModel', title=None, x_bounds=None, y_bounds=None):\n",
    "        \"\"\"Update the metric plot.\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "            title (str, optional): Plot title. Defaults to None\n",
    "            x_bounds (list, optional): x-axis bounds. Defaults to None\n",
    "            y_bounds (list, optional): y-axis bounds. Defaults to None\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=self.figsize)\n",
    "            if self.in_jupyter:\n",
    "                self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        epochs = dfhistory['epoch'] if 'epoch' in dfhistory.columns else []\n",
    "\n",
    "        m1 = \"train_\" + self.metric\n",
    "        if m1 in dfhistory.columns:\n",
    "            train_metrics = dfhistory[m1]\n",
    "            self.graph_ax.plot(epochs, train_metrics, 'bo--', label=m1, clip_on=False)\n",
    "\n",
    "        m2 = 'val_' + self.metric\n",
    "        if m2 in dfhistory.columns:\n",
    "            val_metrics = dfhistory[m2]\n",
    "            self.graph_ax.plot(epochs, val_metrics, 'c^-', label=m2, clip_on=False)\n",
    "\n",
    "        if self.metric in dfhistory.columns:\n",
    "            metric_values = dfhistory[self.metric]\n",
    "            self.graph_ax.plot(epochs, metric_values, 'c^-', label=self.metric, clip_on=False)\n",
    "\n",
    "        self.graph_ax.set_xlabel(\"epoch\")\n",
    "        self.graph_ax.set_ylabel(self.metric)\n",
    "        if title:\n",
    "            self.graph_ax.set_title(title)\n",
    "            if not self.in_jupyter and hasattr(model.EpochRunner, 'progress'):\n",
    "                model.EpochRunner.progress.comment_tail = title\n",
    "        if m1 in dfhistory.columns or m2 in dfhistory.columns or self.metric in dfhistory.columns:\n",
    "            self.graph_ax.legend(loc='best')\n",
    "\n",
    "        if len(epochs) > 0:\n",
    "            best_epoch, best_score = self.get_best_score(model)\n",
    "            self.graph_ax.plot(best_epoch, best_score, 'r*', markersize=15, clip_on=False)\n",
    "\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "        if self.in_jupyter:\n",
    "            self.graph_out.update(self.graph_ax.figure)\n",
    "        self.graph_fig.savefig(self.save_path)\n",
    "        self.plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7eba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.414406Z",
     "iopub.status.busy": "2025-02-28T08:42:34.414137Z",
     "iopub.status.idle": "2025-02-28T08:42:34.452408Z",
     "shell.execute_reply": "2025-02-28T08:42:34.451807Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046846,
     "end_time": "2025-02-28T08:42:34.454117",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.407271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kerasmodel\n",
    "import sys,datetime\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage=\"train\", metrics_dict=None,\n",
    "                 optimizer=None, lr_scheduler=None, **kwargs):\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.stage = net, loss_fn, metrics_dict, stage\n",
    "        self.optimizer, self.lr_scheduler = optimizer, lr_scheduler\n",
    "        self.kwargs = kwargs\n",
    "        self.accelerator = accelerator\n",
    "\n",
    "        # Set the network to training mode during the training stage, and evaluation mode otherwise\n",
    "        if self.stage == 'train':\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        features, labels = batch\n",
    "        # Compute loss\n",
    "        with self.accelerator.autocast():\n",
    "            preds = self.net(features)\n",
    "            loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        # Backward pass and optimization (only during training)\n",
    "        if self.stage == \"train\" and self.optimizer is not None:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "            # Clip gradients if synchronization is enabled\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Adjust learning rate if a scheduler is provided\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next iteration\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # Gather loss, predictions, and labels using the accelerator\n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        all_preds = self.accelerator.gather(preds)\n",
    "        all_labels = self.accelerator.gather(labels)\n",
    "\n",
    "        # Compute and gather additional metrics\n",
    "        step_losses = {self.stage + \"_loss\": all_loss.item()}\n",
    "        step_metrics = {self.stage + \"_\" + name: metric_fn(all_preds, all_labels).item()\n",
    "                        for name, metric_fn in self.metrics_dict.items()}\n",
    "\n",
    "        # Include learning rate in metrics if available\n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "                \n",
    "        return step_losses, step_metrics\n",
    "\n",
    "class EpochRunner:\n",
    "    def __init__(self, step_runner, quiet=False):\n",
    "        self.step_runner = step_runner\n",
    "        self.stage = step_runner.stage\n",
    "        self.accelerator = step_runner.accelerator\n",
    "        self.net = step_runner.net\n",
    "        self.quiet = quiet\n",
    "\n",
    "    def __call__(self, dataloader):\n",
    "        # Determine the size of the dataset\n",
    "        n = dataloader.size if hasattr(dataloader, 'size') else len(dataloader)\n",
    "\n",
    "        # Initialize tqdm progress bar\n",
    "        loop = tqdm(enumerate(dataloader, start=1),\n",
    "                    total=n,\n",
    "                    file=sys.stdout,\n",
    "                    disable=not self.accelerator.is_local_main_process or self.quiet,\n",
    "                    ncols=100\n",
    "                    )\n",
    "        epoch_losses = {}\n",
    "\n",
    "        for step, batch in loop:\n",
    "            # Perform a step with the provided StepRunner\n",
    "            with self.accelerator.accumulate(self.net):\n",
    "                step_losses, step_metrics = self.step_runner(batch)\n",
    "                step_log = dict(step_losses, **step_metrics)\n",
    "\n",
    "                # Accumulate step losses for computing epoch losses\n",
    "                for k, v in step_losses.items():\n",
    "                    epoch_losses[k] = epoch_losses.get(k, 0.0) + v\n",
    "\n",
    "                # Update progress bar during the epoch\n",
    "                if step < n:\n",
    "                    loop.set_postfix(**step_log)\n",
    "\n",
    "                    if hasattr(self, 'progress') and self.accelerator.is_local_main_process:\n",
    "                        post_log = dict(**{'i': step, 'n': n}, **step_log)\n",
    "                        self.progress.set_postfix(**post_log)\n",
    "\n",
    "                # Compute and display epoch-level metrics at the end of the epoch\n",
    "                elif step == n:\n",
    "                    epoch_metrics = step_metrics\n",
    "                    epoch_metrics.update({self.stage + \"_\" + name: metric_fn.compute().item()\n",
    "                                          for name, metric_fn in self.step_runner.metrics_dict.items()})\n",
    "                    epoch_losses = {k: v / step for k, v in epoch_losses.items()}\n",
    "                    epoch_log = dict(epoch_losses, **epoch_metrics)\n",
    "                    loop.set_postfix(**epoch_log)\n",
    "\n",
    "                    # Update progress bar if available\n",
    "                    if hasattr(self, 'progress') and self.accelerator.is_local_main_process:\n",
    "                        post_log = dict(**{'i': step, 'n': n}, **epoch_log)\n",
    "                        self.progress.set_postfix(**post_log)\n",
    "\n",
    "                    # Reset stateful metrics for the next epoch\n",
    "                    for name, metric_fn in self.step_runner.metrics_dict.items():\n",
    "                        metric_fn.reset()\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return epoch_log\n",
    "\n",
    "class KerasModel(torch.nn.Module):\n",
    "    StepRunner, EpochRunner = StepRunner, EpochRunner\n",
    "\n",
    "    def __init__(self, net, loss_fn, metrics_dict=None, optimizer=None, lr_scheduler=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net, self.loss_fn, self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict)\n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n",
    "            self.net.parameters(), lr=3e-4)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.kwargs = kwargs\n",
    "        self.from_scratch = True\n",
    "\n",
    "    def save_ckpt(self, ckpt_path=None, accelerator=None):\n",
    "        accelerator = accelerator if accelerator is not None else self.accelerator\n",
    "        net_dict = accelerator.get_state_dict(self.net)\n",
    "        accelerator.save(net_dict, ckpt_path if ckpt_path is not None else self.ckpt_path)\n",
    "\n",
    "    def load_ckpt(self, ckpt_path=None):\n",
    "        self.net.load_state_dict(torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n",
    "                                            map_location='cpu'))\n",
    "        self.from_scratch = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.forward(x)\n",
    "\n",
    "    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n",
    "            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None,\n",
    "            plot=True, wandb=False, quiet=None,\n",
    "            mixed_precision='no', cpu=False, gradient_accumulation_steps=1):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training data\n",
    "            val_data: Validation data\n",
    "            epochs: Number of training epochs\n",
    "            ckpt_path: Path to save model checkpoints\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            monitor: Metric to monitor for early stopping\n",
    "            mode: 'min' for minimizing the monitor metric, 'max' for maximizing\n",
    "            callbacks: List of callback functions\n",
    "            plot: Whether to plot training progress\n",
    "            wandb: Whether to use WandB for logging\n",
    "            quiet: Whether to suppress training progress logs\n",
    "            mixed_precision: Mixed precision training ('no', 'O1', 'O2', 'O3')\n",
    "            cpu: Use CPU for training\n",
    "            gradient_accumulation_steps: Number of steps to accumulate gradients before optimizer step\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing training history.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(locals())\n",
    "        from accelerate import Accelerator\n",
    "        from torchkeras.utils import colorful, is_jupyter\n",
    "\n",
    "        self.accelerator = Accelerator(mixed_precision=mixed_precision, cpu=cpu,\n",
    "                                       gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "        device = str(self.accelerator.device)\n",
    "        device_type = '🐌' if 'cpu' in device else ('⚡️' if 'cuda' in device else '🚀')\n",
    "        self.accelerator.print(\n",
    "            colorful(\"<<<<<< \" + device_type + \" \" + device + \" is used >>>>>>\"))\n",
    "\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "            self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler)\n",
    "\n",
    "        for key in self.kwargs:\n",
    "            self.kwargs[key] = self.accelerator.prepare(self.kwargs[key])\n",
    "\n",
    "        train_dataloader, val_dataloader = self.accelerator.prepare(train_data, val_data)\n",
    "        train_dataloader.size = train_data.size if hasattr(train_data, 'size') else len(train_data)\n",
    "        train_dataloader.size = min(train_dataloader.size, len(train_dataloader))\n",
    "\n",
    "        if val_data:\n",
    "            val_dataloader.size = val_data.size if hasattr(val_data, 'size') else len(val_data)\n",
    "            val_dataloader.size = min(val_dataloader.size, len(val_dataloader))\n",
    "\n",
    "        self.history = {}\n",
    "        callbacks = callbacks if callbacks is not None else []\n",
    "\n",
    "        if bool(plot):\n",
    "            from torchkeras.kerascallbacks import VisProgress\n",
    "            callbacks = [VisMetric(), VisProgress()] + callbacks\n",
    "\n",
    "        if wandb != False:\n",
    "            from torchkeras.kerascallbacks import WandbCallback\n",
    "            project = wandb if isinstance(wandb, str) else 'torchkeras'\n",
    "            callbacks.append(WandbCallback(project=project))\n",
    "\n",
    "        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n",
    "\n",
    "        if self.accelerator.is_local_main_process:\n",
    "            [cb.on_fit_start(model=self) for cb in self.callbacks if hasattr(cb, 'on_fit_start')]\n",
    "\n",
    "        start_epoch = 1 if self.from_scratch else 0\n",
    "\n",
    "        if bool(plot) or quiet is None:\n",
    "            quiet = True\n",
    "\n",
    "        quiet_fn = (lambda epoch: quiet) if isinstance(quiet, bool) else (\n",
    "            (lambda epoch: epoch > quiet) if isinstance(quiet, int) else quiet)\n",
    "\n",
    "        for epoch in range(start_epoch, epochs + 1):\n",
    "            should_quiet = quiet_fn(epoch)\n",
    "\n",
    "            if not should_quiet:\n",
    "                nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                self.accelerator.print(\"\\n\" + \"==========\" * 8 + \"%s\" % nowtime)\n",
    "                self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs) + \"\\n\")\n",
    "\n",
    "            # 1，train -------------------------------------------------\n",
    "            train_step_runner = self.StepRunner(\n",
    "                net=self.net,\n",
    "                loss_fn=self.loss_fn,\n",
    "                accelerator=self.accelerator,\n",
    "                stage=\"train\",\n",
    "                metrics_dict=deepcopy(self.metrics_dict),\n",
    "                optimizer=self.optimizer if epoch > 0 else None,\n",
    "                lr_scheduler=self.lr_scheduler if epoch > 0 else None,\n",
    "                **self.kwargs\n",
    "            )\n",
    "\n",
    "            train_epoch_runner = self.EpochRunner(train_step_runner, should_quiet)\n",
    "            train_metrics = {'epoch': epoch}\n",
    "            train_metrics.update(train_epoch_runner(train_dataloader))\n",
    "\n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            if self.accelerator.is_local_main_process:\n",
    "                [cb.on_train_epoch_end(model=self) for cb in self.callbacks\n",
    "                 if hasattr(cb, 'on_train_epoch_end')]\n",
    "            # 2，validate -------------------------------------------------\n",
    "            if val_dataloader is not None:\n",
    "                val_step_runner = self.StepRunner(\n",
    "                    net = self.net,\n",
    "                    loss_fn = self.loss_fn,\n",
    "                    accelerator = self.accelerator,\n",
    "                    stage=\"val\",\n",
    "                    metrics_dict= deepcopy(self.metrics_dict),\n",
    "                    **self.kwargs\n",
    "                )\n",
    "                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_dataloader)\n",
    "\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "                \n",
    "            if self.accelerator.is_local_main_process:\n",
    "                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n",
    "                 if hasattr(cb,'on_validation_epoch_end')]\n",
    "\n",
    "            # 3，early-stopping -------------------------------------------------\n",
    "            self.accelerator.wait_for_everyone()\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "\n",
    "            if best_score_idx==len(arr_scores)-1 and self.accelerator.is_local_main_process:\n",
    "                self.save_ckpt(ckpt_path,accelerator = self.accelerator)\n",
    "                if not should_quiet:\n",
    "                    self.accelerator.print(colorful(\"<<<<<< reach best {0} : {1} >>>>>>\".format(\n",
    "                        monitor,arr_scores[best_score_idx])))\n",
    "\n",
    "            if len(arr_scores)-best_score_idx>patience:\n",
    "                break\n",
    "                \n",
    "        if self.accelerator.is_local_main_process:   \n",
    "            dfhistory = pd.DataFrame(self.history)\n",
    "            [cb.on_fit_end(model = self) for cb in self.callbacks \n",
    "                 if hasattr(cb,'on_fit_end')]\n",
    "            if epoch<epochs:\n",
    "                self.accelerator.print(colorful(\n",
    "                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n",
    "                    ).format(monitor,patience))\n",
    "            self.net = self.accelerator.unwrap_model(self.net)\n",
    "            self.net.cpu()\n",
    "            self.load_ckpt(ckpt_path)\n",
    "            return dfhistory\n",
    "        \n",
    "    def evaluate(self, val_data, quiet=False):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation data\n",
    "\n",
    "        Args:\n",
    "            val_data: Validation data\n",
    "            quiet: Whether to suppress evaluation progress logs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Ensure accelerator is available or create a new one\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator() if not hasattr(self, 'accelerator') else self.accelerator\n",
    "\n",
    "        # Prepare model, loss function, and metrics for evaluation\n",
    "        self.net, self.loss_fn, self.metrics_dict = accelerator.prepare(\n",
    "            self.net, self.loss_fn, self.metrics_dict)\n",
    "\n",
    "        val_data = accelerator.prepare(val_data)\n",
    "\n",
    "        # Initialize StepRunner for validation\n",
    "        val_step_runner = self.StepRunner(net=self.net, stage=\"val\",\n",
    "                                          loss_fn=self.loss_fn, metrics_dict=deepcopy(self.metrics_dict),\n",
    "                                          accelerator=accelerator)\n",
    "\n",
    "        # Initialize EpochRunner for validation\n",
    "        val_epoch_runner = self.EpochRunner(val_step_runner, quiet=quiet)\n",
    "\n",
    "        # Evaluate on validation data without gradient computation\n",
    "        with torch.no_grad():\n",
    "            val_metrics = val_epoch_runner(val_data)\n",
    "\n",
    "        return val_metrics\n",
    "    \n",
    "    def fit_ddp(self, num_processes, train_data,\n",
    "                val_data=None, epochs=10, ckpt_path='checkpoint',\n",
    "                patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None,\n",
    "                plot=True, wandb=False, quiet=None,\n",
    "                mixed_precision='no', cpu=False, gradient_accumulation_steps=1):\n",
    "        \"\"\"\n",
    "        Distributed Data Parallel (DDP) training for the model.\n",
    "\n",
    "        Args:\n",
    "            num_processes: Number of processes for DDP\n",
    "            train_data: Training data\n",
    "            val_data: Validation data\n",
    "            epochs: Number of training epochs\n",
    "            ckpt_path: Path to save model checkpoints\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            monitor: Metric to monitor for early stopping\n",
    "            mode: 'min' for minimizing the monitor metric, 'max' for maximizing\n",
    "            callbacks: List of callback functions\n",
    "            plot: Whether to plot training progress\n",
    "            wandb: Whether to use WandB for logging\n",
    "            quiet: Whether to suppress training progress logs\n",
    "            mixed_precision: Mixed precision training ('no', 'O1', 'O2', 'O3')\n",
    "            cpu: Use CPU for training\n",
    "            gradient_accumulation_steps: Number of steps to accumulate gradients before optimizer step\n",
    "        \"\"\"\n",
    "        # Import notebook_launcher from accelerate\n",
    "        from accelerate import notebook_launcher\n",
    "\n",
    "        # Create a tuple of arguments for the fit method\n",
    "        args = (train_data, val_data, epochs, ckpt_path, patience, monitor, mode,\n",
    "                callbacks, plot, wandb, quiet, mixed_precision, cpu, gradient_accumulation_steps)\n",
    "\n",
    "        # Launch the fit method using notebook_launcher\n",
    "        notebook_launcher(self.fit, args, num_processes=num_processes)\n",
    "    \n",
    "    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n",
    "        \"\"\"\n",
    "        Distributed Data Parallel (DDP) evaluation for the model\n",
    "\n",
    "        Args:\n",
    "            num_processes: Number of processes for DDP\n",
    "            val_data: Validation data.\n",
    "            quiet: Whether to suppress evaluation progress logs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Import notebook_launcher from accelerate\n",
    "        from accelerate import notebook_launcher\n",
    "\n",
    "        # Create a tuple of arguments for the evaluate method\n",
    "        args = (val_data, quiet)\n",
    "\n",
    "        # Launch the evaluate method using notebook_launcher\n",
    "        notebook_launcher(self.evaluate, args, num_processes=num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96026b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.466214Z",
     "iopub.status.busy": "2025-02-28T08:42:34.465958Z",
     "iopub.status.idle": "2025-02-28T08:42:34.478340Z",
     "shell.execute_reply": "2025-02-28T08:42:34.477688Z"
    },
    "papermill": {
     "duration": 0.020473,
     "end_time": "2025-02-28T08:42:34.479899",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.459426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class DepthwiseSeparableConv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(DepthwiseSeparableConv1D, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                   groups=in_channels, padding=kernel_size//2)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, num_classes, max_len=5000):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "       #深度可分离卷积降维\n",
    "        self.conv1=DepthwiseSeparableConv1D(in_channels=1,out_channels=32,kernel_size = 11)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool= nn.MaxPool1d(2)#等价于nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2=DepthwiseSeparableConv1D(in_channels=32,out_channels=64,kernel_size = 21)\n",
    "        self.conv3=DepthwiseSeparableConv1D(in_channels=64,out_channels=128,kernel_size = 41)\n",
    "         # 线性投影层，将输入数据5000映射到 embed_dim 维度\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
    "        \n",
    "        # Transformer 编码器层\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=256, dropout=0.2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # 全局池化（或者可以选用 [CLS] token 表示）\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # 最终分类层\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "        #转置卷积forward\n",
    "        x=self.conv1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        \n",
    "        x=self.conv3(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        # 输入 x 的形状：[batch_size, seq_len, input_dim]\n",
    "        x = self.embedding(x)  # 形状变为：[batch_size, seq_len, embed_dim]\n",
    "        x = self.pos_encoder(x)  # 添加位置编码\n",
    "        x = x.permute(1, 0, 2)  # Transformer 期望的输入形状：[seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # 通过 Transformer 编码器\n",
    "        x = self.transformer_encoder(x)  # 输出形状：[seq_len, batch_size, embed_dim]\n",
    "        x = x.permute(1, 2, 0)  # 调整形状为：[batch_size, embed_dim, seq_len]\n",
    "        \n",
    "        # 全局平均池化\n",
    "        x = self.global_avg_pool(x).squeeze(-1)  # 形状变为：[batch_size, embed_dim]\n",
    "        \n",
    "        # 全连接分类层\n",
    "        x = self.fc(x)  # 输出形状：[batch_size, num_classes]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]  # 加上位置编码\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbae1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.492333Z",
     "iopub.status.busy": "2025-02-28T08:42:34.491586Z",
     "iopub.status.idle": "2025-02-28T08:42:34.500932Z",
     "shell.execute_reply": "2025-02-28T08:42:34.500312Z"
    },
    "papermill": {
     "duration": 0.017115,
     "end_time": "2025-02-28T08:42:34.502460",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.485345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from torch.optim import lr_scheduler\n",
    "config = Namespace(\n",
    "    project_name = \"multi-1DDSTN\",\n",
    "    file_path = \"alldata.csv\",\n",
    "    batch_size = 128,\n",
    "    dropout_p = 0.2,\n",
    "    lr = 0.0001,\n",
    "    optim_type = 'Adam',\n",
    "    epochs = 200,\n",
    "    ckpt_path = 'checkpoint',\n",
    "    num_workers=0,\n",
    "    name='transCNN-transformer',\n",
    "    input_dim = 625,  # 转置卷积的输出维度\n",
    "    embed_dim = 256,  # 嵌入维度\n",
    "    num_heads = 8,  # 注意力头数\n",
    "    num_layers = 2,  # Transformer 编码器的层数\n",
    "    num_classes = 14,  # 分类类别数\n",
    "    max_len = 5000  # 输入的最大序列长度\n",
    ")\n",
    "\n",
    "torch.manual_seed(17) #cpu\n",
    "torch.cuda.manual_seed(17) #gpu\n",
    "np.random.seed(17) #numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6131f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.513927Z",
     "iopub.status.busy": "2025-02-28T08:42:34.513677Z",
     "iopub.status.idle": "2025-02-28T08:42:34.812014Z",
     "shell.execute_reply": "2025-02-28T08:42:34.811140Z"
    },
    "papermill": {
     "duration": 0.306569,
     "end_time": "2025-02-28T08:42:34.814344",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.507775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchkeras import summary\n",
    "a=torch.randn([2,1,5000])\n",
    "net = TimeSeriesTransformer(config.input_dim, config.embed_dim, config.num_heads, \n",
    "                            config.num_layers, config.num_classes, config.max_len)\n",
    "summary(net,input_data=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c97b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T12:21:07.749771Z",
     "iopub.status.busy": "2024-11-04T12:21:07.748902Z",
     "iopub.status.idle": "2024-11-04T12:21:07.931491Z",
     "shell.execute_reply": "2024-11-04T12:21:07.930514Z",
     "shell.execute_reply.started": "2024-11-04T12:21:07.749730Z"
    },
    "papermill": {
     "duration": 0.012118,
     "end_time": "2025-02-28T08:42:34.835061",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.822943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244e6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:42:34.861640Z",
     "iopub.status.busy": "2025-02-28T08:42:34.861193Z",
     "iopub.status.idle": "2025-02-28T08:43:09.971733Z",
     "shell.execute_reply": "2025-02-28T08:43:09.970998Z"
    },
    "papermill": {
     "duration": 35.126226,
     "end_time": "2025-02-28T08:43:09.973826",
     "exception": false,
     "start_time": "2025-02-28T08:42:34.847600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,filepath):\n",
    "        self.alldata=pd.read_csv(filepath,header=None)\n",
    "        self.len=self.alldata.shape[0]\n",
    "        self.alldata=np.array(self.alldata,dtype='float32')\n",
    "        self.xdata=torch.from_numpy(self.alldata[:,0:-2])\n",
    "        self.ydata=torch.from_numpy(self.alldata[:,[-1]])##多分类\n",
    "    def __getitem__(self,index):\n",
    "        xx=self.xdata[index]\n",
    "        lb=savgol_filter(xx, window_length=7, polyorder=2)#Savitzky-Golay 平滑滤波器\n",
    "        scaler=MinMaxScaler()\n",
    "        lb=lb.reshape(-1,1)\n",
    "        lb=scaler.fit_transform(lb)#层归一化\n",
    "        lb=lb.reshape(1,-1)\n",
    "        return lb,self.ydata[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "dfdata = MyDataset(config.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeef8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:09.986878Z",
     "iopub.status.busy": "2025-02-28T08:43:09.986582Z",
     "iopub.status.idle": "2025-02-28T08:43:33.913949Z",
     "shell.execute_reply": "2025-02-28T08:43:33.913221Z"
    },
    "papermill": {
     "duration": 23.936136,
     "end_time": "2025-02-28T08:43:33.916033",
     "exception": false,
     "start_time": "2025-02-28T08:43:09.979897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset\n",
    "dfdata.ydata=dfdata.ydata.squeeze(1)#\n",
    "dfdata.ydata=dfdata.ydata.to(dtype=torch.int64) #使用交叉熵做损失函数时\n",
    "dftmp, dftest_raw = train_test_split(dfdata, random_state=40, test_size=0.1)\n",
    "dftrain_raw, dfval_raw = train_test_split(dftmp, random_state=40, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08f428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:33.928933Z",
     "iopub.status.busy": "2025-02-28T08:43:33.928646Z",
     "iopub.status.idle": "2025-02-28T08:43:33.933172Z",
     "shell.execute_reply": "2025-02-28T08:43:33.932430Z"
    },
    "papermill": {
     "duration": 0.012611,
     "end_time": "2025-02-28T08:43:33.934731",
     "exception": false,
     "start_time": "2025-02-28T08:43:33.922120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataloader\n",
    "dl_train =DataLoader(dftrain_raw, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "dl_val =DataLoader(dfval_raw, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "dl_test =DataLoader(dftest_raw, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86a129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:33.946903Z",
     "iopub.status.busy": "2025-02-28T08:43:33.946623Z",
     "iopub.status.idle": "2025-02-28T08:43:33.953555Z",
     "shell.execute_reply": "2025-02-28T08:43:33.952886Z"
    },
    "papermill": {
     "duration": 0.014856,
     "end_time": "2025-02-28T08:43:33.955098",
     "exception": false,
     "start_time": "2025-02-28T08:43:33.940242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class multiAccuracy(nn.Module):\n",
    "    \"\"\"Accuracy for multi-classification task.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Accuracy module.\"\"\"\n",
    "        super().__init__()\n",
    "        # Counters for correct and total predictions\n",
    "        self.correct = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.total = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        preds = preds.argmax(dim=-1)\n",
    "        targets = targets.reshape(-1)\n",
    "        m = (preds == targets).sum()\n",
    "        n = targets.shape[0] \n",
    "        self.correct += m \n",
    "        self.total += n\n",
    "        \n",
    "        return m/n\n",
    "\n",
    "    def compute(self):\n",
    "         return self.correct.float() / self.total \n",
    "\n",
    "    def reset(self):\n",
    "        self.correct -= self.correct\n",
    "        self.total -= self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54623e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:33.966948Z",
     "iopub.status.busy": "2025-02-28T08:43:33.966694Z",
     "iopub.status.idle": "2025-02-28T08:43:33.978563Z",
     "shell.execute_reply": "2025-02-28T08:43:33.977671Z"
    },
    "papermill": {
     "duration": 0.019476,
     "end_time": "2025-02-28T08:43:33.980052",
     "exception": false,
     "start_time": "2025-02-28T08:43:33.960576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for features,labels in dl_train:\n",
    "    break\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(dl_train.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb240ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:33.991973Z",
     "iopub.status.busy": "2025-02-28T08:43:33.991745Z",
     "iopub.status.idle": "2025-02-28T08:43:34.703736Z",
     "shell.execute_reply": "2025-02-28T08:43:34.702158Z"
    },
    "papermill": {
     "duration": 0.721382,
     "end_time": "2025-02-28T08:43:34.706947",
     "exception": false,
     "start_time": "2025-02-28T08:43:33.985565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchkeras import summary\n",
    "summary(net,input_data=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095c7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:34.726973Z",
     "iopub.status.busy": "2025-02-28T08:43:34.725746Z",
     "iopub.status.idle": "2025-02-28T08:43:36.720002Z",
     "shell.execute_reply": "2025-02-28T08:43:36.719088Z"
    },
    "papermill": {
     "duration": 2.006291,
     "end_time": "2025-02-28T08:43:36.721810",
     "exception": false,
     "start_time": "2025-02-28T08:43:34.715519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb \n",
    "wandb.login(key=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6240066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:36.736046Z",
     "iopub.status.busy": "2025-02-28T08:43:36.735755Z",
     "iopub.status.idle": "2025-02-28T08:43:37.102820Z",
     "shell.execute_reply": "2025-02-28T08:43:37.101893Z"
    },
    "papermill": {
     "duration": 0.376519,
     "end_time": "2025-02-28T08:43:37.104832",
     "exception": false,
     "start_time": "2025-02-28T08:43:36.728313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchkeras import KerasModel\n",
    "#对多分类模型来说，要用Macro Average（宏平均）或Micro Average（微平均）规则来进行F1（或者P、R）的计算。\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score#使用函数调转真实值与预测值的位置precision_score(labels, preds, average='macro')\n",
    "# net = create_net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(net.parameters(),lr=config.lr)\n",
    "# lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "metric_dict = {\"acc\":multiAccuracy()}\n",
    "model = KerasModel(net,\n",
    "                   loss_fn = loss_fn,\n",
    "                   metrics_dict= metric_dict,\n",
    "                   optimizer = optimizer\n",
    "#                    ,\n",
    "#                    lr_scheduler=lr_scheduler\n",
    "                  )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db564c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:37.118604Z",
     "iopub.status.busy": "2025-02-28T08:43:37.118299Z",
     "iopub.status.idle": "2025-02-28T08:43:37.124297Z",
     "shell.execute_reply": "2025-02-28T08:43:37.123686Z"
    },
    "papermill": {
     "duration": 0.014614,
     "end_time": "2025-02-28T08:43:37.125903",
     "exception": false,
     "start_time": "2025-02-28T08:43:37.111289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchkeras.kerascallbacks import WandbCallback\n",
    "wandb_cb = WandbCallback(project=config.project_name,\n",
    "                         config=config,\n",
    "                         name=None,\n",
    "                         save_code=True,\n",
    "                         save_ckpt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c20bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:43:37.139026Z",
     "iopub.status.busy": "2025-02-28T08:43:37.138762Z",
     "iopub.status.idle": "2025-02-28T09:07:35.374866Z",
     "shell.execute_reply": "2025-02-28T09:07:35.373555Z"
    },
    "papermill": {
     "duration": 1438.24493,
     "end_time": "2025-02-28T09:07:35.376934",
     "exception": false,
     "start_time": "2025-02-28T08:43:37.132004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfhistory = model.fit(\n",
    "      train_data=dl_train,\n",
    "      val_data=dl_val,\n",
    "      epochs=config.epochs,\n",
    "      ckpt_path='checkpoint',\n",
    "      patience=15,\n",
    "      monitor='val_acc',\n",
    "      mode='max',\n",
    "      callbacks = [wandb_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820756f0",
   "metadata": {
    "papermill": {
     "duration": 0.007682,
     "end_time": "2025-02-28T09:07:35.392534",
     "exception": false,
     "start_time": "2025-02-28T09:07:35.384852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5940593,
     "sourceId": 9711824,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1560.154728,
   "end_time": "2025-02-28T09:07:38.935672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-28T08:41:38.780944",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
