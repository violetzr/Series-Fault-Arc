{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bf0e2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:05.804920Z",
     "iopub.status.busy": "2025-03-02T10:16:05.804591Z",
     "iopub.status.idle": "2025-03-02T10:16:15.872022Z",
     "shell.execute_reply": "2025-03-02T10:16:15.871056Z"
    },
    "papermill": {
     "duration": 10.075402,
     "end_time": "2025-03-02T10:16:15.874045",
     "exception": false,
     "start_time": "2025-03-02T10:16:05.798643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0420d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:15.885491Z",
     "iopub.status.busy": "2025-03-02T10:16:15.884819Z",
     "iopub.status.idle": "2025-03-02T10:16:38.578412Z",
     "shell.execute_reply": "2025-03-02T10:16:38.577521Z"
    },
    "papermill": {
     "duration": 22.701332,
     "end_time": "2025-03-02T10:16:38.580556",
     "exception": false,
     "start_time": "2025-03-02T10:16:15.879224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/lyhue1991/torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda62d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:38.592316Z",
     "iopub.status.busy": "2025-03-02T10:16:38.592003Z",
     "iopub.status.idle": "2025-03-02T10:16:58.119626Z",
     "shell.execute_reply": "2025-03-02T10:16:58.118712Z"
    },
    "papermill": {
     "duration": 19.535875,
     "end_time": "2025-03-02T10:16:58.121789",
     "exception": false,
     "start_time": "2025-03-02T10:16:38.585914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import savgol_filter #滤波\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchkeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3e036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:58.135074Z",
     "iopub.status.busy": "2025-03-02T10:16:58.134469Z",
     "iopub.status.idle": "2025-03-02T10:16:58.151461Z",
     "shell.execute_reply": "2025-03-02T10:16:58.150649Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.024793,
     "end_time": "2025-03-02T10:16:58.153060",
     "exception": false,
     "start_time": "2025-03-02T10:16:58.128267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainmodel VisMetri\n",
    "from torchkeras.utils import is_jupyter\n",
    "class VisMetric:\n",
    "    def __init__(self, figsize=(6, 4), save_path='history.png'):\n",
    "        \"\"\"Visualization callback for monitoring metrics\n",
    "\n",
    "        Args:\n",
    "            figsize (tuple, optional): Figure size. Defaults to (6, 4)\n",
    "            save_path (str, optional): Path to save the history plot. Defaults to 'history.png'\n",
    "        \"\"\"\n",
    "        self.figsize = (6, 4)\n",
    "        self.save_path = save_path\n",
    "        self.in_jupyter = is_jupyter()\n",
    "\n",
    "    def on_fit_start(self, model: 'KerasModel'):\n",
    "        \"\"\"Callback at the beginning of the training\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance.\n",
    "        \"\"\"\n",
    "        if not self.in_jupyter:\n",
    "            print('\\nView dynamic loss/metric plot: \\n' + os.path.abspath(self.save_path))\n",
    "        # //////////////////////////////////////\n",
    "        self.metric = model.monitor.replace('val_', '')\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        x_bounds = [0, min(10, model.epochs)]\n",
    "        title = f'best {model.monitor} = ?'\n",
    "        self.update_graph(model, title=title, x_bounds=x_bounds)\n",
    "\n",
    "    def on_train_epoch_end(self, model: 'KerasModel'):\n",
    "        \"\"\"Callback at the end of each training epoch\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self, model: \"KerasModel\"):\n",
    "        \"\"\"Callback at the end of each validation epoch\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        n = len(dfhistory)\n",
    "        x_bounds = [dfhistory['epoch'].min(), min(10 + (n // 10) * 10, model.epochs)]\n",
    "        title = self.get_title(model)\n",
    "        self.update_graph(model, title=title, x_bounds=x_bounds)\n",
    "\n",
    "    def on_fit_end(self, model: \"KerasModel\"):\n",
    "        \"\"\"Callback at the end of the entire training process\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        title = self.get_title(model)\n",
    "        self.update_graph(model, title=title)\n",
    "\n",
    "    def get_best_score(self, model: 'KerasModel'):\n",
    "        \"\"\"Get the best score and epoch.\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "\n",
    "        Returns:\n",
    "            tuple: Best epoch and best score\n",
    "        \"\"\"\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        arr_scores = dfhistory[model.monitor]\n",
    "        best_score = np.max(arr_scores) if model.mode == \"max\" else np.min(arr_scores)\n",
    "        best_epoch = dfhistory.loc[arr_scores == best_score, 'epoch'].tolist()[0]\n",
    "        return (best_epoch, best_score)\n",
    "\n",
    "    def get_title(self, model: 'KerasModel'):\n",
    "        \"\"\"Get the title for the plot\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "\n",
    "        Returns:\n",
    "            str: The title.\n",
    "        \"\"\"\n",
    "        best_epoch, best_score = self.get_best_score(model)\n",
    "        title = f'best {model.monitor}={best_score:.4f} (@epoch {best_epoch})'\n",
    "        return title\n",
    "\n",
    "    def update_graph(self, model: 'KerasModel', title=None, x_bounds=None, y_bounds=None):\n",
    "        \"\"\"Update the metric plot.\n",
    "\n",
    "        Args:\n",
    "            model (KerasModel): The KerasModel instance\n",
    "            title (str, optional): Plot title. Defaults to None\n",
    "            x_bounds (list, optional): x-axis bounds. Defaults to None\n",
    "            y_bounds (list, optional): y-axis bounds. Defaults to None\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=self.figsize)\n",
    "            if self.in_jupyter:\n",
    "                self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "\n",
    "        dfhistory = pd.DataFrame(model.history)\n",
    "        epochs = dfhistory['epoch'] if 'epoch' in dfhistory.columns else []\n",
    "\n",
    "        m1 = \"train_\" + self.metric\n",
    "        if m1 in dfhistory.columns:\n",
    "            train_metrics = dfhistory[m1]\n",
    "            self.graph_ax.plot(epochs, train_metrics, 'bo--', label=m1, clip_on=False)\n",
    "\n",
    "        m2 = 'val_' + self.metric\n",
    "        if m2 in dfhistory.columns:\n",
    "            val_metrics = dfhistory[m2]\n",
    "            self.graph_ax.plot(epochs, val_metrics, 'c^-', label=m2, clip_on=False)\n",
    "\n",
    "        if self.metric in dfhistory.columns:\n",
    "            metric_values = dfhistory[self.metric]\n",
    "            self.graph_ax.plot(epochs, metric_values, 'c^-', label=self.metric, clip_on=False)\n",
    "\n",
    "        self.graph_ax.set_xlabel(\"epoch\")\n",
    "        self.graph_ax.set_ylabel(self.metric)\n",
    "        if title:\n",
    "            self.graph_ax.set_title(title)\n",
    "            if not self.in_jupyter and hasattr(model.EpochRunner, 'progress'):\n",
    "                model.EpochRunner.progress.comment_tail = title\n",
    "        if m1 in dfhistory.columns or m2 in dfhistory.columns or self.metric in dfhistory.columns:\n",
    "            self.graph_ax.legend(loc='best')\n",
    "\n",
    "        if len(epochs) > 0:\n",
    "            best_epoch, best_score = self.get_best_score(model)\n",
    "            self.graph_ax.plot(best_epoch, best_score, 'r*', markersize=15, clip_on=False)\n",
    "\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "        if self.in_jupyter:\n",
    "            self.graph_out.update(self.graph_ax.figure)\n",
    "        self.graph_fig.savefig(self.save_path)\n",
    "        self.plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3208b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:58.164581Z",
     "iopub.status.busy": "2025-03-02T10:16:58.164313Z",
     "iopub.status.idle": "2025-03-02T10:16:58.201784Z",
     "shell.execute_reply": "2025-03-02T10:16:58.200933Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.045534,
     "end_time": "2025-03-02T10:16:58.203625",
     "exception": false,
     "start_time": "2025-03-02T10:16:58.158091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kerasmodel\n",
    "import sys,datetime\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage=\"train\", metrics_dict=None,\n",
    "                 optimizer=None, lr_scheduler=None, **kwargs):\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.stage = net, loss_fn, metrics_dict, stage\n",
    "        self.optimizer, self.lr_scheduler = optimizer, lr_scheduler\n",
    "        self.kwargs = kwargs\n",
    "        self.accelerator = accelerator\n",
    "\n",
    "        # Set the network to training mode during the training stage, and evaluation mode otherwise\n",
    "        if self.stage == 'train':\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        features, labels = batch\n",
    "        # Compute loss\n",
    "        with self.accelerator.autocast():\n",
    "            preds = self.net(features)\n",
    "            loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        # Backward pass and optimization (only during training)\n",
    "        if self.stage == \"train\" and self.optimizer is not None:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "            # Clip gradients if synchronization is enabled\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Adjust learning rate if a scheduler is provided\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next iteration\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # Gather loss, predictions, and labels using the accelerator\n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        all_preds = self.accelerator.gather(preds)\n",
    "        all_labels = self.accelerator.gather(labels)\n",
    "\n",
    "        # Compute and gather additional metrics\n",
    "        step_losses = {self.stage + \"_loss\": all_loss.item()}\n",
    "        step_metrics = {self.stage + \"_\" + name: metric_fn(all_preds, all_labels).item()\n",
    "                        for name, metric_fn in self.metrics_dict.items()}\n",
    "\n",
    "        # Include learning rate in metrics if available\n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "                \n",
    "        return step_losses, step_metrics\n",
    "\n",
    "class EpochRunner:\n",
    "    def __init__(self, step_runner, quiet=False):\n",
    "        self.step_runner = step_runner\n",
    "        self.stage = step_runner.stage\n",
    "        self.accelerator = step_runner.accelerator\n",
    "        self.net = step_runner.net\n",
    "        self.quiet = quiet\n",
    "\n",
    "    def __call__(self, dataloader):\n",
    "        # Determine the size of the dataset\n",
    "        n = dataloader.size if hasattr(dataloader, 'size') else len(dataloader)\n",
    "\n",
    "        # Initialize tqdm progress bar\n",
    "        loop = tqdm(enumerate(dataloader, start=1),\n",
    "                    total=n,\n",
    "                    file=sys.stdout,\n",
    "                    disable=not self.accelerator.is_local_main_process or self.quiet,\n",
    "                    ncols=100\n",
    "                    )\n",
    "        epoch_losses = {}\n",
    "\n",
    "        for step, batch in loop:\n",
    "            # Perform a step with the provided StepRunner\n",
    "            with self.accelerator.accumulate(self.net):\n",
    "                step_losses, step_metrics = self.step_runner(batch)\n",
    "                step_log = dict(step_losses, **step_metrics)\n",
    "\n",
    "                # Accumulate step losses for computing epoch losses\n",
    "                for k, v in step_losses.items():\n",
    "                    epoch_losses[k] = epoch_losses.get(k, 0.0) + v\n",
    "\n",
    "                # Update progress bar during the epoch\n",
    "                if step < n:\n",
    "                    loop.set_postfix(**step_log)\n",
    "\n",
    "                    if hasattr(self, 'progress') and self.accelerator.is_local_main_process:\n",
    "                        post_log = dict(**{'i': step, 'n': n}, **step_log)\n",
    "                        self.progress.set_postfix(**post_log)\n",
    "\n",
    "                # Compute and display epoch-level metrics at the end of the epoch\n",
    "                elif step == n:\n",
    "                    epoch_metrics = step_metrics\n",
    "                    epoch_metrics.update({self.stage + \"_\" + name: metric_fn.compute().item()\n",
    "                                          for name, metric_fn in self.step_runner.metrics_dict.items()})\n",
    "                    epoch_losses = {k: v / step for k, v in epoch_losses.items()}\n",
    "                    epoch_log = dict(epoch_losses, **epoch_metrics)\n",
    "                    loop.set_postfix(**epoch_log)\n",
    "\n",
    "                    # Update progress bar if available\n",
    "                    if hasattr(self, 'progress') and self.accelerator.is_local_main_process:\n",
    "                        post_log = dict(**{'i': step, 'n': n}, **epoch_log)\n",
    "                        self.progress.set_postfix(**post_log)\n",
    "\n",
    "                    # Reset stateful metrics for the next epoch\n",
    "                    for name, metric_fn in self.step_runner.metrics_dict.items():\n",
    "                        metric_fn.reset()\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return epoch_log\n",
    "\n",
    "class KerasModel(torch.nn.Module):\n",
    "    StepRunner, EpochRunner = StepRunner, EpochRunner\n",
    "\n",
    "    def __init__(self, net, loss_fn, metrics_dict=None, optimizer=None, lr_scheduler=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net, self.loss_fn, self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict)\n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n",
    "            self.net.parameters(), lr=3e-4)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.kwargs = kwargs\n",
    "        self.from_scratch = True\n",
    "\n",
    "    def save_ckpt(self, ckpt_path=None, accelerator=None):\n",
    "        accelerator = accelerator if accelerator is not None else self.accelerator\n",
    "        net_dict = accelerator.get_state_dict(self.net)\n",
    "        accelerator.save(net_dict, ckpt_path if ckpt_path is not None else self.ckpt_path)\n",
    "\n",
    "    def load_ckpt(self, ckpt_path=None):\n",
    "        self.net.load_state_dict(torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n",
    "                                            map_location='cpu'))\n",
    "        self.from_scratch = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.forward(x)\n",
    "\n",
    "    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n",
    "            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None,\n",
    "            plot=True, wandb=False, quiet=None,\n",
    "            mixed_precision='no', cpu=False, gradient_accumulation_steps=1):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Args:\n",
    "            train_data: Training data\n",
    "            val_data: Validation data\n",
    "            epochs: Number of training epochs\n",
    "            ckpt_path: Path to save model checkpoints\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            monitor: Metric to monitor for early stopping\n",
    "            mode: 'min' for minimizing the monitor metric, 'max' for maximizing\n",
    "            callbacks: List of callback functions\n",
    "            plot: Whether to plot training progress\n",
    "            wandb: Whether to use WandB for logging\n",
    "            quiet: Whether to suppress training progress logs\n",
    "            mixed_precision: Mixed precision training ('no', 'O1', 'O2', 'O3')\n",
    "            cpu: Use CPU for training\n",
    "            gradient_accumulation_steps: Number of steps to accumulate gradients before optimizer step\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing training history.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(locals())\n",
    "        from accelerate import Accelerator\n",
    "        from torchkeras.utils import colorful, is_jupyter\n",
    "\n",
    "        self.accelerator = Accelerator(mixed_precision=mixed_precision, cpu=cpu,\n",
    "                                       gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "        device = str(self.accelerator.device)\n",
    "        device_type = '🐌' if 'cpu' in device else ('⚡️' if 'cuda' in device else '🚀')\n",
    "        self.accelerator.print(\n",
    "            colorful(\"<<<<<< \" + device_type + \" \" + device + \" is used >>>>>>\"))\n",
    "\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "            self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler)\n",
    "\n",
    "        for key in self.kwargs:\n",
    "            self.kwargs[key] = self.accelerator.prepare(self.kwargs[key])\n",
    "\n",
    "        train_dataloader, val_dataloader = self.accelerator.prepare(train_data, val_data)\n",
    "        train_dataloader.size = train_data.size if hasattr(train_data, 'size') else len(train_data)\n",
    "        train_dataloader.size = min(train_dataloader.size, len(train_dataloader))\n",
    "\n",
    "        if val_data:\n",
    "            val_dataloader.size = val_data.size if hasattr(val_data, 'size') else len(val_data)\n",
    "            val_dataloader.size = min(val_dataloader.size, len(val_dataloader))\n",
    "\n",
    "        self.history = {}\n",
    "        callbacks = callbacks if callbacks is not None else []\n",
    "\n",
    "        if bool(plot):\n",
    "            from torchkeras.kerascallbacks import VisProgress\n",
    "            callbacks = [VisMetric(), VisProgress()] + callbacks\n",
    "\n",
    "        if wandb != False:\n",
    "            from torchkeras.kerascallbacks import WandbCallback\n",
    "            project = wandb if isinstance(wandb, str) else 'torchkeras'\n",
    "            callbacks.append(WandbCallback(project=project))\n",
    "\n",
    "        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n",
    "\n",
    "        if self.accelerator.is_local_main_process:\n",
    "            [cb.on_fit_start(model=self) for cb in self.callbacks if hasattr(cb, 'on_fit_start')]\n",
    "\n",
    "        start_epoch = 1 if self.from_scratch else 0\n",
    "\n",
    "        if bool(plot) or quiet is None:\n",
    "            quiet = True\n",
    "\n",
    "        quiet_fn = (lambda epoch: quiet) if isinstance(quiet, bool) else (\n",
    "            (lambda epoch: epoch > quiet) if isinstance(quiet, int) else quiet)\n",
    "\n",
    "        for epoch in range(start_epoch, epochs + 1):\n",
    "            should_quiet = quiet_fn(epoch)\n",
    "\n",
    "            if not should_quiet:\n",
    "                nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                self.accelerator.print(\"\\n\" + \"==========\" * 8 + \"%s\" % nowtime)\n",
    "                self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs) + \"\\n\")\n",
    "\n",
    "            # 1，train -------------------------------------------------\n",
    "            train_step_runner = self.StepRunner(\n",
    "                net=self.net,\n",
    "                loss_fn=self.loss_fn,\n",
    "                accelerator=self.accelerator,\n",
    "                stage=\"train\",\n",
    "                metrics_dict=deepcopy(self.metrics_dict),\n",
    "                optimizer=self.optimizer if epoch > 0 else None,\n",
    "                lr_scheduler=self.lr_scheduler if epoch > 0 else None,\n",
    "                **self.kwargs\n",
    "            )\n",
    "\n",
    "            train_epoch_runner = self.EpochRunner(train_step_runner, should_quiet)\n",
    "            train_metrics = {'epoch': epoch}\n",
    "            train_metrics.update(train_epoch_runner(train_dataloader))\n",
    "\n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            if self.accelerator.is_local_main_process:\n",
    "                [cb.on_train_epoch_end(model=self) for cb in self.callbacks\n",
    "                 if hasattr(cb, 'on_train_epoch_end')]\n",
    "            # 2，validate -------------------------------------------------\n",
    "            if val_dataloader is not None:\n",
    "                val_step_runner = self.StepRunner(\n",
    "                    net = self.net,\n",
    "                    loss_fn = self.loss_fn,\n",
    "                    accelerator = self.accelerator,\n",
    "                    stage=\"val\",\n",
    "                    metrics_dict= deepcopy(self.metrics_dict),\n",
    "                    **self.kwargs\n",
    "                )\n",
    "                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_dataloader)\n",
    "\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "                \n",
    "            if self.accelerator.is_local_main_process:\n",
    "                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n",
    "                 if hasattr(cb,'on_validation_epoch_end')]\n",
    "\n",
    "            # 3，early-stopping -------------------------------------------------\n",
    "            self.accelerator.wait_for_everyone()\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "\n",
    "            if best_score_idx==len(arr_scores)-1 and self.accelerator.is_local_main_process:\n",
    "                self.save_ckpt(ckpt_path,accelerator = self.accelerator)\n",
    "                if not should_quiet:\n",
    "                    self.accelerator.print(colorful(\"<<<<<< reach best {0} : {1} >>>>>>\".format(\n",
    "                        monitor,arr_scores[best_score_idx])))\n",
    "\n",
    "            if len(arr_scores)-best_score_idx>patience:\n",
    "                break\n",
    "                \n",
    "        if self.accelerator.is_local_main_process:   \n",
    "            dfhistory = pd.DataFrame(self.history)\n",
    "            [cb.on_fit_end(model = self) for cb in self.callbacks \n",
    "                 if hasattr(cb,'on_fit_end')]\n",
    "            if epoch<epochs:\n",
    "                self.accelerator.print(colorful(\n",
    "                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n",
    "                    ).format(monitor,patience))\n",
    "            self.net = self.accelerator.unwrap_model(self.net)\n",
    "            self.net.cpu()\n",
    "            self.load_ckpt(ckpt_path)\n",
    "            return dfhistory\n",
    "        \n",
    "    def evaluate(self, val_data, quiet=False):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation data\n",
    "\n",
    "        Args:\n",
    "            val_data: Validation data\n",
    "            quiet: Whether to suppress evaluation progress logs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Ensure accelerator is available or create a new one\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator() if not hasattr(self, 'accelerator') else self.accelerator\n",
    "\n",
    "        # Prepare model, loss function, and metrics for evaluation\n",
    "        self.net, self.loss_fn, self.metrics_dict = accelerator.prepare(\n",
    "            self.net, self.loss_fn, self.metrics_dict)\n",
    "\n",
    "        val_data = accelerator.prepare(val_data)\n",
    "\n",
    "        # Initialize StepRunner for validation\n",
    "        val_step_runner = self.StepRunner(net=self.net, stage=\"val\",\n",
    "                                          loss_fn=self.loss_fn, metrics_dict=deepcopy(self.metrics_dict),\n",
    "                                          accelerator=accelerator)\n",
    "\n",
    "        # Initialize EpochRunner for validation\n",
    "        val_epoch_runner = self.EpochRunner(val_step_runner, quiet=quiet)\n",
    "\n",
    "        # Evaluate on validation data without gradient computation\n",
    "        with torch.no_grad():\n",
    "            val_metrics = val_epoch_runner(val_data)\n",
    "\n",
    "        return val_metrics\n",
    "    \n",
    "    def fit_ddp(self, num_processes, train_data,\n",
    "                val_data=None, epochs=10, ckpt_path='checkpoint',\n",
    "                patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None,\n",
    "                plot=True, wandb=False, quiet=None,\n",
    "                mixed_precision='no', cpu=False, gradient_accumulation_steps=1):\n",
    "        \"\"\"\n",
    "        Distributed Data Parallel (DDP) training for the model.\n",
    "\n",
    "        Args:\n",
    "            num_processes: Number of processes for DDP\n",
    "            train_data: Training data\n",
    "            val_data: Validation data\n",
    "            epochs: Number of training epochs\n",
    "            ckpt_path: Path to save model checkpoints\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            monitor: Metric to monitor for early stopping\n",
    "            mode: 'min' for minimizing the monitor metric, 'max' for maximizing\n",
    "            callbacks: List of callback functions\n",
    "            plot: Whether to plot training progress\n",
    "            wandb: Whether to use WandB for logging\n",
    "            quiet: Whether to suppress training progress logs\n",
    "            mixed_precision: Mixed precision training ('no', 'O1', 'O2', 'O3')\n",
    "            cpu: Use CPU for training\n",
    "            gradient_accumulation_steps: Number of steps to accumulate gradients before optimizer step\n",
    "        \"\"\"\n",
    "        # Import notebook_launcher from accelerate\n",
    "        from accelerate import notebook_launcher\n",
    "\n",
    "        # Create a tuple of arguments for the fit method\n",
    "        args = (train_data, val_data, epochs, ckpt_path, patience, monitor, mode,\n",
    "                callbacks, plot, wandb, quiet, mixed_precision, cpu, gradient_accumulation_steps)\n",
    "\n",
    "        # Launch the fit method using notebook_launcher\n",
    "        notebook_launcher(self.fit, args, num_processes=num_processes)\n",
    "    \n",
    "    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n",
    "        \"\"\"\n",
    "        Distributed Data Parallel (DDP) evaluation for the model\n",
    "\n",
    "        Args:\n",
    "            num_processes: Number of processes for DDP\n",
    "            val_data: Validation data.\n",
    "            quiet: Whether to suppress evaluation progress logs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Import notebook_launcher from accelerate\n",
    "        from accelerate import notebook_launcher\n",
    "\n",
    "        # Create a tuple of arguments for the evaluate method\n",
    "        args = (val_data, quiet)\n",
    "\n",
    "        # Launch the evaluate method using notebook_launcher\n",
    "        notebook_launcher(self.evaluate, args, num_processes=num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030fb86e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:58.215203Z",
     "iopub.status.busy": "2025-03-02T10:16:58.214927Z",
     "iopub.status.idle": "2025-03-02T10:16:58.229800Z",
     "shell.execute_reply": "2025-03-02T10:16:58.228979Z"
    },
    "papermill": {
     "duration": 0.0225,
     "end_time": "2025-03-02T10:16:58.231338",
     "exception": false,
     "start_time": "2025-03-02T10:16:58.208838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn')\n",
    "\n",
    "class Inception(torch.nn.Module):\n",
    "    def __init__(self, input_size, filters):\n",
    "        super(Inception, self).__init__()\n",
    "#         瓶颈层用于减少维度或保持维度不变以便进行后续操作\n",
    "# 当stride=1时，padding='SAME'意味着卷积后的输出与输入size保持一致\n",
    "        self.bottleneck1 = torch.nn.Conv1d(\n",
    "            in_channels=input_size,\n",
    "            out_channels=filters,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            bias=False\n",
    "        )\n",
    "# 不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征。        \n",
    "        self.conv20 = torch.nn.Conv1d(\n",
    "            in_channels=filters,\n",
    "            out_channels=filters,\n",
    "            kernel_size=20,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            dilation=5,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.conv40 = torch.nn.Conv1d(\n",
    "            in_channels=filters,\n",
    "            out_channels=filters,\n",
    "            kernel_size=40,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            dilation=5,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.conv60 = torch.nn.Conv1d(\n",
    "            in_channels=filters,\n",
    "            out_channels=filters,\n",
    "            kernel_size=60,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            dilation=5,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.max_pool = torch.nn.MaxPool1d(\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        \n",
    "        self.bottleneck2 = torch.nn.Conv1d(\n",
    "            in_channels=input_size,\n",
    "            out_channels=filters,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            bias=False\n",
    "        )\n",
    "#         当input的维度为（N, C）时，BN将对C维归一化；当input的维度为(N, C, L) 时，归一化的维度同样为C维。\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(\n",
    "            num_features=4 * filters\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.bottleneck1(x)\n",
    "        x1 = self.conv20(x0)\n",
    "        x2 = self.conv40(x0)\n",
    "        x3 = self.conv60(x0)\n",
    "        x4 = self.bottleneck2(self.max_pool(x))\n",
    "        y = torch.concat([x1, x2, x3, x4], dim=1)\n",
    "        y = torch.nn.functional.relu(self.batch_norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, input_size, filters):\n",
    "        super(Residual, self).__init__()\n",
    "        \n",
    "        self.bottleneck = torch.nn.Conv1d(\n",
    "            in_channels=input_size,\n",
    "            out_channels=4 * filters,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding='same',\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(\n",
    "            num_features=4 * filters\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        y = y + self.batch_norm(self.bottleneck(x))\n",
    "        y = torch.nn.functional.relu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Lambda(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "\n",
    "class InceptionModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, filters, depth):\n",
    "        super(InceptionModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "        self.drop=nn.Dropout(p=0.4)\n",
    "        modules = OrderedDict()\n",
    "        \n",
    "        for d in range(depth):\n",
    "            modules[f'inception_{d}'] = Inception(\n",
    "                input_size=input_size if d == 0 else 4 * filters,\n",
    "                filters=filters,\n",
    "            )\n",
    "            if d % 3 == 2:\n",
    "                modules[f'residual_{d}'] = Residual(\n",
    "                    input_size=input_size if d == 2 else 4 * filters,\n",
    "                    filters=filters,\n",
    "                )\n",
    "        \n",
    "        modules['avg_pool'] = Lambda(f=lambda x: torch.mean(x, dim=-1))\n",
    "        # modules['linear1'] = torch.nn.Linear(in_features=4 * filters, out_features=num_classes)\n",
    "        modules['linear1'] = torch.nn.Linear(in_features=4 * filters, out_features=filters)\n",
    "        modules['linear2'] = torch.nn.Linear(in_features=filters, out_features=num_classes)\n",
    "#         modules['linear3'] = torch.nn.Linear(in_features=filters, out_features=num_classes)\n",
    "        self.model = torch.nn.Sequential(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for d in range(self.depth):\n",
    "            y = self.model.get_submodule(f'inception_{d}')(x if d == 0 else y)\n",
    "            if d % 3 == 2:\n",
    "                y = self.model.get_submodule(f'residual_{d}')(x, y)\n",
    "                x = y\n",
    "        y = self.model.get_submodule('avg_pool')(y)\n",
    "        y = self.model.get_submodule('linear1')(y)\n",
    "        y = self.drop(F.relu(y))\n",
    "        y = self.model.get_submodule('linear2')(y)\n",
    "#         y = F.relu(self.drop(self.model.get_submodule('linear1')(y)))\n",
    "#         y = F.relu(self.drop(self.model.get_submodule('linear2')(y)))\n",
    "#         y = self.model.get_submodule('linear3')(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530b79d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:58.242482Z",
     "iopub.status.busy": "2025-03-02T10:16:58.242227Z",
     "iopub.status.idle": "2025-03-02T10:16:58.356681Z",
     "shell.execute_reply": "2025-03-02T10:16:58.355789Z"
    },
    "papermill": {
     "duration": 0.12213,
     "end_time": "2025-03-02T10:16:58.358502",
     "exception": false,
     "start_time": "2025-03-02T10:16:58.236372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchkeras import summary\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net=InceptionModel(\n",
    "                input_size=1,\n",
    "                num_classes=14,\n",
    "                filters=32,\n",
    "                depth=6\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32dec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:16:58.370275Z",
     "iopub.status.busy": "2025-03-02T10:16:58.369518Z",
     "iopub.status.idle": "2025-03-02T10:17:00.035783Z",
     "shell.execute_reply": "2025-03-02T10:17:00.034884Z"
    },
    "papermill": {
     "duration": 1.673895,
     "end_time": "2025-03-02T10:17:00.037563",
     "exception": false,
     "start_time": "2025-03-02T10:16:58.363668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb \n",
    "wandb.login(key=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2ffee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:00.049939Z",
     "iopub.status.busy": "2025-03-02T10:17:00.049626Z",
     "iopub.status.idle": "2025-03-02T10:17:00.058044Z",
     "shell.execute_reply": "2025-03-02T10:17:00.057424Z"
    },
    "papermill": {
     "duration": 0.016344,
     "end_time": "2025-03-02T10:17:00.059614",
     "exception": false,
     "start_time": "2025-03-02T10:17:00.043270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from torch.optim import lr_scheduler\n",
    "config = Namespace(\n",
    "    project_name = \"multi-1DDITN\",\n",
    "    file_path = \"alldata.csv\",\n",
    "    batch_size = 32,\n",
    "    # dropout_p = 0.2,\n",
    "    lr = 1e-3,\n",
    "    optim_type = 'Adam',\n",
    "    epochs = 200,\n",
    "    ckpt_path = 'checkpoint',\n",
    "    num_workers=0,\n",
    "    name='multi-class'\n",
    ")\n",
    "\n",
    "torch.manual_seed(17) #cpu\n",
    "torch.cuda.manual_seed(17) #gpu\n",
    "np.random.seed(17) #numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89cf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:00.071348Z",
     "iopub.status.busy": "2025-03-02T10:17:00.070827Z",
     "iopub.status.idle": "2025-03-02T10:17:27.689817Z",
     "shell.execute_reply": "2025-03-02T10:17:27.689057Z"
    },
    "papermill": {
     "duration": 27.626827,
     "end_time": "2025-03-02T10:17:27.691849",
     "exception": false,
     "start_time": "2025-03-02T10:17:00.065022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,filepath):\n",
    "        self.alldata=pd.read_csv(filepath,header=None)\n",
    "        self.len=self.alldata.shape[0]\n",
    "        self.alldata=np.array(self.alldata,dtype='float32')\n",
    "        self.xdata=torch.from_numpy(self.alldata[:,0:-2])\n",
    "        self.ydata=torch.from_numpy(self.alldata[:,[-1]])##多分类\n",
    "    def __getitem__(self,index):\n",
    "        xx=self.xdata[index]\n",
    "        lb=savgol_filter(xx, window_length=7, polyorder=2)#Savitzky-Golay 平滑滤波器\n",
    "        scaler=MinMaxScaler()\n",
    "        lb=lb.reshape(-1,1)\n",
    "        lb=scaler.fit_transform(lb)#层归一化\n",
    "        lb=lb.reshape(1,-1)\n",
    "        return lb,self.ydata[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "dfdata = MyDataset(config.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26caeb08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:27.704018Z",
     "iopub.status.busy": "2025-03-02T10:17:27.703706Z",
     "iopub.status.idle": "2025-03-02T10:17:50.490655Z",
     "shell.execute_reply": "2025-03-02T10:17:50.489485Z"
    },
    "papermill": {
     "duration": 22.795964,
     "end_time": "2025-03-02T10:17:50.493433",
     "exception": false,
     "start_time": "2025-03-02T10:17:27.697469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset\n",
    "dfdata.ydata=dfdata.ydata.squeeze(1)#\n",
    "dfdata.ydata=dfdata.ydata.to(dtype=torch.int64) #使用交叉熵做损失函数时\n",
    "dftmp, dftest_raw = train_test_split(dfdata, random_state=40, test_size=0.1)\n",
    "dftrain_raw, dfval_raw = train_test_split(dftmp, random_state=40, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d1ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:50.508096Z",
     "iopub.status.busy": "2025-03-02T10:17:50.507626Z",
     "iopub.status.idle": "2025-03-02T10:17:50.513711Z",
     "shell.execute_reply": "2025-03-02T10:17:50.512600Z"
    },
    "papermill": {
     "duration": 0.016048,
     "end_time": "2025-03-02T10:17:50.517112",
     "exception": false,
     "start_time": "2025-03-02T10:17:50.501064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataloader\n",
    "dl_train =DataLoader(dftrain_raw, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "dl_val =DataLoader(dfval_raw, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "dl_test =DataLoader(dftest_raw, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d4900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:50.532204Z",
     "iopub.status.busy": "2025-03-02T10:17:50.531511Z",
     "iopub.status.idle": "2025-03-02T10:17:50.537403Z",
     "shell.execute_reply": "2025-03-02T10:17:50.536772Z"
    },
    "papermill": {
     "duration": 0.013591,
     "end_time": "2025-03-02T10:17:50.538903",
     "exception": false,
     "start_time": "2025-03-02T10:17:50.525312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class multiAccuracy(nn.Module):\n",
    "    \"\"\"Accuracy for multi-classification task.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Accuracy module.\"\"\"\n",
    "        super().__init__()\n",
    "        # Counters for correct and total predictions\n",
    "        self.correct = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.total = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        preds = preds.argmax(dim=-1)\n",
    "        targets = targets.reshape(-1)\n",
    "        m = (preds == targets).sum()\n",
    "        n = targets.shape[0] \n",
    "        self.correct += m \n",
    "        self.total += n\n",
    "        \n",
    "        return m/n\n",
    "\n",
    "    def compute(self):\n",
    "         return self.correct.float() / self.total \n",
    "\n",
    "    def reset(self):\n",
    "        self.correct -= self.correct\n",
    "        self.total -= self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ebc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:50.550402Z",
     "iopub.status.busy": "2025-03-02T10:17:50.549962Z",
     "iopub.status.idle": "2025-03-02T10:17:50.620998Z",
     "shell.execute_reply": "2025-03-02T10:17:50.620093Z"
    },
    "papermill": {
     "duration": 0.078784,
     "end_time": "2025-03-02T10:17:50.622812",
     "exception": false,
     "start_time": "2025-03-02T10:17:50.544028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for features,labels in dl_val:\n",
    "    break\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(dl_train.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecf426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:50.635039Z",
     "iopub.status.busy": "2025-03-02T10:17:50.634478Z",
     "iopub.status.idle": "2025-03-02T10:17:54.128893Z",
     "shell.execute_reply": "2025-03-02T10:17:54.126860Z"
    },
    "papermill": {
     "duration": 3.502424,
     "end_time": "2025-03-02T10:17:54.130792",
     "exception": false,
     "start_time": "2025-03-02T10:17:50.628368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(net,input_data=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d96f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:54.143810Z",
     "iopub.status.busy": "2025-03-02T10:17:54.143267Z",
     "iopub.status.idle": "2025-03-02T10:17:54.545319Z",
     "shell.execute_reply": "2025-03-02T10:17:54.544395Z"
    },
    "papermill": {
     "duration": 0.410608,
     "end_time": "2025-03-02T10:17:54.547367",
     "exception": false,
     "start_time": "2025-03-02T10:17:54.136759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchkeras import KerasModel\n",
    "#对多分类模型来说，要用Macro Average（宏平均）或Micro Average（微平均）规则来进行F1（或者P、R）的计算。\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score#使用函数调转真实值与预测值的位置precision_score(labels, preds, average='macro')\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# net = create_net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(net.parameters(),lr=config.lr)\n",
    "lr_scheduler2 = StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "metric_dict = {\"acc\":multiAccuracy()}\n",
    "model = KerasModel(net,\n",
    "                   loss_fn = loss_fn,\n",
    "                   metrics_dict= metric_dict,\n",
    "                   optimizer = optimizer\n",
    "                   # ,\n",
    "                   # lr_scheduler=lr_scheduler2\n",
    "                  )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81131d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:54.560302Z",
     "iopub.status.busy": "2025-03-02T10:17:54.560019Z",
     "iopub.status.idle": "2025-03-02T10:17:54.564850Z",
     "shell.execute_reply": "2025-03-02T10:17:54.564133Z"
    },
    "papermill": {
     "duration": 0.013119,
     "end_time": "2025-03-02T10:17:54.566458",
     "exception": false,
     "start_time": "2025-03-02T10:17:54.553339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchkeras.kerascallbacks import WandbCallback\n",
    "wandb_cb = WandbCallback(project=config.project_name,\n",
    "                         config=config,\n",
    "                         name=None,\n",
    "                         save_code=True,\n",
    "                         save_ckpt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd54406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:17:54.578545Z",
     "iopub.status.busy": "2025-03-02T10:17:54.578291Z",
     "iopub.status.idle": "2025-03-02T15:49:31.817465Z",
     "shell.execute_reply": "2025-03-02T15:49:31.816470Z"
    },
    "papermill": {
     "duration": 19897.247506,
     "end_time": "2025-03-02T15:49:31.819457",
     "exception": false,
     "start_time": "2025-03-02T10:17:54.571951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfhistory = model.fit(\n",
    "      train_data=dl_train,\n",
    "      val_data=dl_val,\n",
    "      epochs=config.epochs,\n",
    "      ckpt_path='checkpoint',\n",
    "      patience=15,\n",
    "      monitor='val_acc',\n",
    "      mode='max',\n",
    "      callbacks = [wandb_cb]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5940593,
     "isSourceIdPinned": false,
     "sourceId": 10855665,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20012.131028,
   "end_time": "2025-03-02T15:49:35.453271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-02T10:16:03.322243",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
